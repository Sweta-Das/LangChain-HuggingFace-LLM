{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtUZnZGbbtfuAX2gy2g/I+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sweta-Das/LangChain-HuggingFace-LLM/blob/SentenceTransformers/AnalysisOfTextSimilarityMatrices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "%pip -q install PyPDF2 pdfplumber langchain sentence-transformers transformers numba"
      ],
      "metadata": {
        "id": "PuvZmtVHriI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install llama-cpp-python"
      ],
      "metadata": {
        "id": "ZpLWUodiZcC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain import HuggingFaceHub\n",
        "from PyPDF2 import PdfReader\n",
        "from numba import jit, cuda\n",
        "from pdfplumber import pdf\n",
        "import numpy as np\n",
        "import sys, random\n",
        "import torch\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "PW_taBfosms0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About Libraries**:<br>\n",
        "- *RecursiveCharacterTextSplitter* : a function to split text into smaller chunks based on a specified character set & chunk size. Recursive splitting works by repeatedly splitting the text into smaller pieces until it reaches a desired size or encounters a separator character.\n",
        "- *SentenceTransformer* : class used for embedding sentences into numerical vectors for various NLP tasks\n",
        "- *LlamaCPP* : a LangChain's wrapper class that enables to use the Llama LLM within LangChain.\n",
        "- *LLMChain* : a LangChain's class specifically designed to interact with LLMs\n",
        "- *HuggingFaceHub* :  class that joins LangChain with Hugging Face\n",
        "- *PyPDF2* : a library that works with PDF files in Python; *PdfReader* reads the PDF docs' content\n",
        "- *pdfplumber* : a library for extracting text & data from PDF docs; *pdf* works with PDFs\n",
        "- *numba* : a library in Python ecosystem used for high-performance numerical computing. It provides **JIT (Just In Time)** compiler *(@jit)* that translates Python functions into optimized machine code at runtime. It also support **cuda** like *(@cuda.jit)* to execute code on NVIDIA GPUs.\n",
        "\n"
      ],
      "metadata": {
        "id": "A6bv-q7JnJeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A2BvaPcCY_A2"
      },
      "outputs": [],
      "source": [
        "# Accessing through HuggingFace Access Token\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'HUGGINGFACEHUB_API_TOKEN'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive/')\n",
        "model = 'drive/MyDrive/LLM_Model/mistral-7b-instruct-v0.1.Q3_K_S.gguf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blu4VjL9Nit4",
        "outputId": "cc082e46-d1b3-4cfc-d15e-6319aca1f18b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def progressBar(count_value, total, suffix=''):\n",
        "  # Designing progress bar (==---)\n",
        "  bar_length = 100\n",
        "  filled_up_length = int(round(bar_length * count_value / float(total)))\n",
        "  percent = round(100.0 * count_value/float(total), 1)\n",
        "  bar = '=' * filled_up_length + '-' * (bar_length - filled_up_length)\n",
        "  sys.stdout.write('[%s] %s%s ...%s\\r' %(bar, percent, '%', suffix))\n",
        "  sys.stdout.flush()"
      ],
      "metadata": {
        "id": "2mG6VWS9t2F8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading the pdf file"
      ],
      "metadata": {
        "id": "NAWhLrFHvvmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_split_pdf(pdf_path):\n",
        "  # Reading pdf in binary mode\n",
        "  pdf_loader = PdfReader(open(pdf_path, \"rb\"))\n",
        "  pdf_text = \"\"\n",
        "\n",
        "  # Reading only 8 pages of pdf\n",
        "  for page_num in range(len(pdf_loader.pages)): # min(8, len(pdf_loader.pages))\n",
        "    # Loading page\n",
        "    pdf_page = pdf_loader.pages[page_num]\n",
        "    # Extracting text\n",
        "    pdf_text += pdf_page.extract_text()\n",
        "\n",
        "  progressBar(2, 7)\n",
        "  return pdf_text"
      ],
      "metadata": {
        "id": "U6LtFpK9u_aK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recursive Text Character Splitter"
      ],
      "metadata": {
        "id": "JqQyvMdevgjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_using_RCTS(pdf_text):\n",
        "\n",
        "  # Splitting text recursively\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = 2048,\n",
        "      chunk_overlap = 64\n",
        "  )\n",
        "  split_texts = text_splitter.split_text(pdf_text)\n",
        "\n",
        "  # Separating texts at paragraphs\n",
        "  paragraphs = []\n",
        "  for text in split_texts:\n",
        "    paragraphs.extend(text.split('\\n'))\n",
        "\n",
        "  progressBar(3, 7)\n",
        "  return paragraphs"
      ],
      "metadata": {
        "id": "xKVNOU6PvDOp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Transformer"
      ],
      "metadata": {
        "id": "HvjvmrnJv2ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing sentence transformer\n",
        "def Initialize_sentence_transformer():\n",
        "  model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "  embeddings = SentenceTransformer(model_name)\n",
        "\n",
        "  progressBar(4, 7)\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "fvdfwBtavFYh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding each paragraph\n",
        "def encode_each_paragraph(paragraphs, embeddings):\n",
        "  responses = []\n",
        "  for paragraph in paragraphs:\n",
        "    response = embeddings.encode([paragraph], convert_to_tensor=True)\n",
        "    responses.append((paragraph, response))\n",
        "\n",
        "  progressBar(5, 7)\n",
        "  return responses"
      ],
      "metadata": {
        "id": "Yi2XXS4gvIZZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing most relevant sentence\n",
        "def choose_most_relevant_sentence_using_CosineSimilarity(embeddings, responses, query):\n",
        "  query_embedding = embeddings.encode([query], convert_to_tensor=True)\n",
        "  best_response = None\n",
        "  best_similarity = -1.0\n",
        "  answers = []\n",
        "\n",
        "  for paragraph, response in responses:\n",
        "    # Finding cosine similarity between query embedding and response\n",
        "    similarity = util.pytorch_cos_sim(query_embedding, response).item()\n",
        "\n",
        "    if similarity >= 0.8:\n",
        "      # count += 1\n",
        "      answers.append(paragraph)\n",
        "\n",
        "  answer = \"\\n\".join(answers)\n",
        "\n",
        "  progressBar(6, 7)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "PCdDQy9ywBQz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying the LLM"
      ],
      "metadata": {
        "id": "nOhbZqLMwSsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query():\n",
        "    query = input(\"Enter your question\\n\")\n",
        "    progressBar(1, 7)\n",
        "    return query"
      ],
      "metadata": {
        "id": "G1qj_Kxswti4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_the_llm(answer, llm_model, query):\n",
        "    prompt_message = answer + \"\\n\" + query\n",
        "\n",
        "    final_response = llm_model.generate(prompts=[prompt_message])\n",
        "\n",
        "    return final_response"
      ],
      "metadata": {
        "id": "z3qbfLq3wQhc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine Similarity"
      ],
      "metadata": {
        "id": "ZAplvk5y4nZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the LLM Model\n",
        "def main():\n",
        "  start_time = time.time()\n",
        "  pdf_path = \"./HandbookOfTechnicalAnalysis.pdf\"\n",
        "  pdf_text = load_split_pdf(pdf_path)\n",
        "  paragraphs = split_text_using_RCTS(pdf_text)\n",
        "  embeddings = Initialize_sentence_transformer()\n",
        "  responses = encode_each_paragraph(paragraphs=paragraphs, embeddings=embeddings)\n",
        "  # print(responses)\n",
        "  query = get_query()\n",
        "  answer = choose_most_relevant_sentence_using_CosineSimilarity(embeddings=embeddings, responses=responses, query=query)\n",
        "\n",
        "  llm = LlamaCpp(\n",
        "      streaming = True,\n",
        "      model_path = \"/content/drive/MyDrive/LLM_Model/mistral-7b-instruct-v0.1.Q3_K_S.gguf\",\n",
        "      temperature = 0.75, # degree of randomness\n",
        "      top_p = 1,\n",
        "      verbose = True,\n",
        "      n_ctx = 4096 # max no. of tokens to generate\n",
        "  )\n",
        "\n",
        "  final_response = query_the_llm(answer=answer, llm_model=llm, query=query)\n",
        "\n",
        "  print (\"The answer from model is\\n\", final_response)\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"Execution time: {elapsed_time/60} minutes \\n\")\n",
        "\n",
        "  progressBar(7, 7)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "4IGKtgMaujNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f150570-1234-49a6-bc38-579ff84d8aa5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question\n",
            "Explain about number of registered users in elearnmarket.com\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /content/drive/MyDrive/LLM_Model/mistral-7b-instruct-v0.1.Q3_K_S.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 11\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q3_K:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q3_K - Small\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 2.95 GiB (3.50 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  3017.27 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 8\n",
            "llama_new_context_with_model: n_ubatch   = 8\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     4.63 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '11'}\n",
            "Using fallback chat format: None\n",
            "\n",
            "llama_print_timings:        load time =    4979.31 ms\n",
            "llama_print_timings:      sample time =      84.56 ms /   127 runs   (    0.67 ms per token,  1501.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9384.97 ms /    16 tokens (  586.56 ms per token,     1.70 tokens per second)\n",
            "llama_print_timings:        eval time =   94231.29 ms /   126 runs   (  747.87 ms per token,     1.34 tokens per second)\n",
            "llama_print_timings:       total time =  104237.08 ms /   142 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer from model is\n",
            " generations=[[Generation(text='.\\nNumber of registered users on Elearnmarket.com refers to the total number of individuals who have created an account with the website by providing their personal information, such as name, email address, and a password. These users are able to access the platform’s features and services, including purchasing and enrolling in online courses, interacting with instructors and peers, and managing their account settings. The number of registered users on Elearnmarket.com may fluctuate over time due to factors such as new user sign-ups, existing users discontinuing their accounts, and changes in the platform’s marketing efforts.')]] llm_output=None run=[RunInfo(run_id=UUID('1affb2c4-deb1-472b-9dd5-07369279f298'))]\n",
            "Execution time: 3.106291969617208 minutes \n",
            "\n",
            "[====================================================================================================] 100.0% ...\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters within LlamaCPP: <br>\n",
        "- *streaming* : controls how the LLM processes and generates text. Set to True (by default).  Handles input text in a continuous stream, generating output word by word or in small chunks.\n",
        "<br>\n",
        "\n",
        "- *temperature* : controls the **level of randomness**; higher temperature (closer to 1) leads to more creative and diverse outputs, but potentially less coherence, while  lower temperature (closer to 0) results in more predictable and consistent outputs that are more likely to follow the established patterns in the training data\n",
        "<br>\n",
        "\n",
        "- *top-p* : **nucleus sampling**; influences which words the LLM is more likely to choose during generation, value of '1' means the LLM considers all possible words in its vocabulary, but prioritizes those with higher probabilities according to its internal model. Higher values (closer to the total vocabulary size) would restrict the selection to a smaller set of high-probability words, potentially leading to less creative but more focused outputs.\n",
        "<br>\n",
        "\n",
        "- *verbose* : controls whether the LLM prints additional information during its operation; <br>\n",
        "Set to **True** leads to output messages about its progress, warnings or errors; helpful for debugging or behavior monitoring. <br>\n",
        "Set tp **False** suppresses these messages for clean execution.\n",
        "<br>\n",
        "\n",
        "- *n_ctx* : specifies the max. no. of tokens (word or subword units) considered by LLM generating text"
      ],
      "metadata": {
        "id": "5_bNZXhCvr4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Euclidean Distance"
      ],
      "metadata": {
        "id": "bpqEl_FE4uet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def choose_most_relevant_sentence_using_EuclideanDistance(embeddings, responses, query):\n",
        "  query_embedding = embeddings.encode([query], convert_to_tensor=True)\n",
        "  best_response = None\n",
        "  best_similarity = -1.0\n",
        "  answers = []\n",
        "\n",
        "  for paragraph, response in responses:\n",
        "    # Finding Euclidean distance between query embedding and response\n",
        "    euc_dist = np.linalg.norm(query_embedding - response)\n",
        "    similarity = 1 / (1 + euc_dist)\n",
        "\n",
        "    if similarity <= 0.3:\n",
        "      # count += 1\n",
        "      answers.append(paragraph)\n",
        "\n",
        "  answer = \"\\n\".join(answers)\n",
        "\n",
        "  progressBar(6, 7)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "MnyHNtTX3ZAV"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "pdf_path = \"./HandbookOfTechnicalAnalysis.pdf\"\n",
        "pdf_text = load_split_pdf(pdf_path)\n",
        "paragraphs = split_text_using_RCTS(pdf_text)\n",
        "embeddings = Initialize_sentence_transformer()\n",
        "responses = encode_each_paragraph(paragraphs=paragraphs, embeddings=embeddings)\n",
        "# print(responses)\n",
        "query = get_query()\n",
        "answer = choose_most_relevant_sentence_using_EuclideanDistance(embeddings=embeddings, responses=responses, query=query)\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    streaming = True,\n",
        "    model_path = \"/content/drive/MyDrive/LLM_Model/mistral-7b-instruct-v0.1.Q3_K_S.gguf\",\n",
        "    temperature = 0.75, # degree of randomness\n",
        "    top_p = 1,\n",
        "    verbose = False,\n",
        "    n_ctx = 4096 # max no. of tokens to generate\n",
        ")\n",
        "\n",
        "final_response = query_the_llm(answer=answer, llm_model=llm, query=query)\n",
        "\n",
        "print (\"The answer from model is\\n\", final_response)\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Execution time: {elapsed_time/60} minutes \\n\")\n",
        "\n",
        "progressBar(7, 7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKlg3KGO3KH-",
        "outputId": "bc8b41b9-821f-46b7-b8fc-ebefe1b380e7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question\n",
            "Explain about registered users in elearnmarket.com\n",
            "The answer from model is\n",
            " generations=[[Generation(text='.\\nA registered user is someone who has created an account on ElearnMarket.com by providing their personal information, including a valid email address and password. Once registered, users can access various features and services available on the platform, such as creating and managing online courses, posting and searching for jobs, and participating in discussion forums.\\n\\nRegistered users also have the ability to earn points and rewards by completing various tasks on the site, such as taking quizzes or attending webinars. These points can be redeemed for prizes or used to purchase additional features and services.\\n\\nIn addition, registered users can also track their progress and performance through detailed analytics and reporting tools available on the platform. This allows them to better understand their strengths and areas for improvement, and make informed decisions about their learning and career development goals.\\n\\nOverall, being a registered user on ElearnMarket.com provides access to a wide range of valuable resources and opportunities for personal and professional growth.')]] llm_output=None run=[RunInfo(run_id=UUID('c276549a-f2dc-42f8-8b1b-ebe507f37d32'))]\n",
            "Execution time: 3.535266403357188 minutes \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manhattan Distance"
      ],
      "metadata": {
        "id": "Zjrwjdrs436G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def choose_most_relevant_sentence_using_ManhattanDistance(embeddings, responses, query):\n",
        "  query_embedding = embeddings.encode([query], convert_to_tensor=True)\n",
        "  best_response = None\n",
        "  best_similarity = -1.0\n",
        "  answers = []\n",
        "\n",
        "  for paragraph, response in responses:\n",
        "    # Finding Manhattan Distance between query embedding and response\n",
        "    man_dist = np.linalg.norm(query_embedding - response, ord=1)\n",
        "    similarity = 1 / (1 + man_dist)\n",
        "\n",
        "    if similarity <= 0.3:\n",
        "      # count += 1\n",
        "      answers.append(paragraph)\n",
        "\n",
        "  answer = \"\\n\".join(answers)\n",
        "\n",
        "  progressBar(6, 7)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "KYpuclzU4loe"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "pdf_path = \"./HandbookOfTechnicalAnalysis.pdf\"\n",
        "pdf_text = load_split_pdf(pdf_path)\n",
        "paragraphs = split_text_using_RCTS(pdf_text)\n",
        "embeddings = Initialize_sentence_transformer()\n",
        "responses = encode_each_paragraph(paragraphs=paragraphs, embeddings=embeddings)\n",
        "# print(responses)\n",
        "query = get_query()\n",
        "answer = choose_most_relevant_sentence_using_ManhattanDistance(embeddings=embeddings, responses=responses, query=query)\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    streaming = True,\n",
        "    model_path = \"/content/drive/MyDrive/LLM_Model/mistral-7b-instruct-v0.1.Q3_K_S.gguf\",\n",
        "    temperature = 0.75, # degree of randomness\n",
        "    top_p = 1,\n",
        "    verbose = False,\n",
        "    n_ctx = 4096 # max no. of tokens to generate\n",
        ")\n",
        "\n",
        "final_response = query_the_llm(answer=answer, llm_model=llm, query=query)\n",
        "\n",
        "print (\"The answer from model is\\n\", final_response)\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Execution time: {elapsed_time/60} minutes \\n\")\n",
        "\n",
        "progressBar(7, 7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT8Q3MAP46Gv",
        "outputId": "7ea13279-10da-4465-b402-b1235281b4d2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question\n",
            "Explain about registered users in elearnmarket.com\n",
            "The answer from model is\n",
            " generations=[[Generation(text='\\n\\nWhat is meant by registered users in elearnmarket.com? Are these users who have created accounts on the website or are they those who have completed their courses and certifications?')]] llm_output=None run=[RunInfo(run_id=UUID('c1db927b-a70a-47e9-a9e4-662b10e5a8c0'))]\n",
            "Execution time: 1.1008305033047994 minutes \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dot Product"
      ],
      "metadata": {
        "id": "0XG60onR5VB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def choose_most_relevant_sentence_using_DotProduct(embeddings, responses, query):\n",
        "  query_embedding = embeddings.encode([query], convert_to_tensor=True)\n",
        "\n",
        "  best_response = None\n",
        "  best_similarity = -1.0\n",
        "  answers = []\n",
        "\n",
        "  for paragraph, response in responses:\n",
        "    # Reshaping query embedding to match the shape of response\n",
        "    # query_embedding = query_embedding.view(response.shape)\n",
        "\n",
        "    # Finding dot product between query embedding and response\n",
        "    dot_product = torch.matmul(query_embedding, response.T) # np.dot(query_embedding, response)\n",
        "    similarity = dot_product\n",
        "\n",
        "    if similarity >= 0.8:\n",
        "      # count += 1\n",
        "      answers.append(paragraph)\n",
        "\n",
        "  answer = \"\\n\".join(answers)\n",
        "\n",
        "  progressBar(6, 7)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "NQ6Zl65_5XYV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "pdf_path = \"./HandbookOfTechnicalAnalysis.pdf\"\n",
        "pdf_text = load_split_pdf(pdf_path)\n",
        "paragraphs = split_text_using_RCTS(pdf_text)\n",
        "embeddings = Initialize_sentence_transformer()\n",
        "responses = encode_each_paragraph(paragraphs=paragraphs, embeddings=embeddings)\n",
        "# print(responses)\n",
        "query = get_query()\n",
        "answer = choose_most_relevant_sentence_using_DotProduct(embeddings=embeddings, responses=responses, query=query)\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    streaming = True,\n",
        "    model_path = \"/content/drive/MyDrive/LLM_Model/mistral-7b-instruct-v0.1.Q3_K_S.gguf\",\n",
        "    temperature = 0.4, # degree of randomness\n",
        "    top_p = 1,\n",
        "    verbose = False,\n",
        "    n_ctx = 4096 # max no. of tokens to generate\n",
        ")\n",
        "\n",
        "final_response = query_the_llm(answer=answer, llm_model=llm, query=query)\n",
        "\n",
        "print (\"The answer from model is\\n\", final_response)\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Execution time: {elapsed_time/60} minutes \\n\")\n",
        "\n",
        "progressBar(7, 7)"
      ],
      "metadata": {
        "id": "VSI4HWNE5ZAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995c0dc1-f4ff-4557-e998-f530653a19d6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question\n",
            "Explain about registered users in elearnmarket.com\n",
            "The answer from model is\n",
            " generations=[[Generation(text=\"\\n\\nElearnmarket.com is an online learning platform that offers a wide range of courses and training programs to help individuals and organizations improve their skills and knowledge. Registered users on the platform are those who have created an account with their personal information, including their name, email address, and password. Once registered, users can access the platform's features, such as creating profiles, enrolling in courses, tracking progress, and receiving certificates of completion. Additionally, registered users can also interact with other users on the platform by participating in discussion forums and sharing course content. Overall, being a registered user on elearnmarket.com allows individuals to take advantage of the platform's many benefits and improve their learning experience.\")]] llm_output=None run=[RunInfo(run_id=UUID('cf9e7564-8bf1-4640-9190-6636962e6ea4'))]\n",
            "Execution time: 2.9183318773905436 minutes \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Angular Similarity"
      ],
      "metadata": {
        "id": "dmbTr3bC62mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def choose_most_relevant_sentence_using_AngularSimilarity(embeddings, responses, query):\n",
        "  query_embedding = embeddings.encode([query], convert_to_tensor=True)\n",
        "  best_response = None\n",
        "  best_similarity = -1.0\n",
        "  answers = []\n",
        "\n",
        "  for paragraph, response in responses:\n",
        "    # Finding cosine similarity between query embedding and response\n",
        "    dot_product = np.dot(query_embedding, response) / (np.linalg.norm(query_embedding) * np.linalg.norm(response))\n",
        "    similarity = dot_product\n",
        "\n",
        "    if similarity >= 0.8:\n",
        "      # count += 1\n",
        "      answers.append(paragraph)\n",
        "\n",
        "  answer = \"\\n\".join(answers)\n",
        "\n",
        "  progressBar(6, 7)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "VkQASU0C6470"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "pdf_path = \"./HandbookOfTechnicalAnalysis.pdf\"\n",
        "pdf_text = load_split_pdf(pdf_path)\n",
        "paragraphs = split_text_using_RCTS(pdf_text)\n",
        "embeddings = Initialize_sentence_transformer()\n",
        "responses = encode_each_paragraph(paragraphs=paragraphs, embeddings=embeddings)\n",
        "# print(responses)\n",
        "query = get_query()\n",
        "answer = choose_most_relevant_sentence_using_AngularSimilarity(embeddings=embeddings, responses=responses, query=query)\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    streaming = True,\n",
        "    model_path = \"/content/drive/MyDrive/LLM_Model/mistral-7b-instruct-v0.1.Q3_K_S.gguf\",\n",
        "    temperature = 0.4, # degree of randomness\n",
        "    top_p = 1,\n",
        "    verbose = False,\n",
        "    n_ctx = 4096 # max no. of tokens to generate\n",
        ")\n",
        "\n",
        "final_response = query_the_llm(answer=answer, llm_model=llm, query=query)\n",
        "\n",
        "print (\"The answer from model is\\n\", final_response)\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Execution time: {elapsed_time/60} minutes \\n\")\n",
        "\n",
        "progressBar(7, 7)"
      ],
      "metadata": {
        "id": "ve4Z0lR5-yMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Referenced From: <br>\n",
        "[**Handbook**](https://www.elearnmarkets.com/uploads/content_pdf/MxudjQevBU.pdf)<br>\n",
        "[**Querying a PDF file using LLM models and Sentence transformer**](https://medium.com/@yashashm77/querying-a-pdf-file-using-llm-models-and-sentence-transformer-b3d4d0b40f7d)<br>"
      ],
      "metadata": {
        "id": "5yIn5gOsxbQO"
      }
    }
  ]
}