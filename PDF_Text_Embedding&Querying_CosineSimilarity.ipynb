{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5EFbaY2+NTRZTt3sTiC1l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sweta-Das/LangChain-HuggingFace-LLM/blob/SentenceTransformers/PDF_Text_Embedding%26Querying_CosineSimilarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "%pip -q install PyPDF2 pdfplumber langchain sentence-transformers transformers numba"
      ],
      "metadata": {
        "id": "PuvZmtVHriI1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install llama-cpp-python"
      ],
      "metadata": {
        "id": "ZpLWUodiZcC_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain import HuggingFaceHub\n",
        "from PyPDF2 import PdfReader\n",
        "from numba import jit, cuda\n",
        "from pdfplumber import pdf\n",
        "import numpy as np\n",
        "import sys, random\n",
        "import torch\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "PW_taBfosms0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About Libraries**:<br>\n",
        "- *RecursiveCharacterTextSplitter* : a function to split text into smaller chunks based on a specified character set & chunk size. Recursive splitting works by repeatedly splitting the text into smaller pieces until it reaches a desired size or encounters a separator character.\n",
        "- *SentenceTransformer* : class used for embedding sentences into numerical vectors for various NLP tasks\n",
        "- *LLMChain* : a LangChain's class specifically designed to interact with LLMs\n",
        "- *HuggingFaceHub* :  class that joins LangChain with Hugging Face\n",
        "- *PyPDF2* : a library that works with PDF files in Python; *PdfReader* reads the PDF docs' content\n",
        "- *pdfplumber* : a library for extracting text & data from PDF docs; *pdf* works with PDFs\n",
        "- *numba* : a library in Python ecosystem used for high-performance numerical computing. It provides **JIT (Just In Time)** compiler *(@jit)* that translates Python functions into optimized machine code at runtime. It also support **cuda** like *(@cuda.jit)* to execute code on NVIDIA GPUs.\n",
        "\n"
      ],
      "metadata": {
        "id": "A6bv-q7JnJeC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "A2BvaPcCY_A2"
      },
      "outputs": [],
      "source": [
        "# Accessing through HuggingFace Access Token\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'HUGGINGFACEHUB_API_TOKEN'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive/')\n",
        "model = 'drive/MyDrive/LLM_Model/mistral-7b-instruct-v0.1.Q3_K_S.gguf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blu4VjL9Nit4",
        "outputId": "ad6c932f-83d4-478f-ca3e-37d84cd98dce"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def progressBar(count_value, total, suffix=''):\n",
        "  # Designing progress bar (==---)\n",
        "  bar_length = 100\n",
        "  filled_up_length = int(round(bar_length * count_value / float(total)))\n",
        "  percent = round(100.0 * count_value/float(total), 1)\n",
        "  bar = '=' * filled_up_length + '-' * (bar_length - filled_up_length)\n",
        "  sys.stdout.write('[%s] %s%s ...%s\\r' %(bar, percent, '%', suffix))\n",
        "  sys.stdout.flush()"
      ],
      "metadata": {
        "id": "2mG6VWS9t2F8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading the pdf file"
      ],
      "metadata": {
        "id": "NAWhLrFHvvmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_split_pdf(pdf_path):\n",
        "  # Reading pdf in binary mode\n",
        "  pdf_loader = PdfReader(open(pdf_path, \"rb\"))\n",
        "  pdf_text = \"\"\n",
        "\n",
        "  # Reading only 8 pages of pdf\n",
        "  for page_num in range(min(8, len(pdf_loader.pages))): # len(pdf_loader.pages)\n",
        "    # Loading page\n",
        "    pdf_page = pdf_loader.pages[page_num]\n",
        "    # Extracting text\n",
        "    pdf_text += pdf_page.extract_text()\n",
        "\n",
        "  last_page_num = len(pdf_loader.pages) - 1\n",
        "  if last_page_num >= 0:\n",
        "    last_page = pdf_loader.pages[last_page_num]\n",
        "    pdf_text += last_page.extract_text()\n",
        "  progressBar(2, 7)\n",
        "  return pdf_text"
      ],
      "metadata": {
        "id": "U6LtFpK9u_aK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recursive Text Character Splitter"
      ],
      "metadata": {
        "id": "JqQyvMdevgjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_using_RCTS(pdf_text):\n",
        "\n",
        "  # Splitting text recursively\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = 2048,\n",
        "      chunk_overlap = 64\n",
        "  )\n",
        "  split_texts = text_splitter.split_text(pdf_text)\n",
        "\n",
        "  # Separating texts at paragraphs\n",
        "  paragraphs = []\n",
        "  for text in split_texts:\n",
        "    paragraphs.extend(text.split('\\n'))\n",
        "\n",
        "  progressBar(3, 7)\n",
        "  return paragraphs"
      ],
      "metadata": {
        "id": "xKVNOU6PvDOp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Transformer"
      ],
      "metadata": {
        "id": "HvjvmrnJv2ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing sentence transformer\n",
        "def Initialize_sentence_transformer():\n",
        "  model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "  embeddings = SentenceTransformer(model_name)\n",
        "\n",
        "  progressBar(4, 7)\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "fvdfwBtavFYh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding each paragraph\n",
        "def encode_each_paragraph(paragraphs, embeddings):\n",
        "  responses = []\n",
        "  for paragraph in paragraphs:\n",
        "    response = embeddings.encode([paragraph], convert_to_tensor=True)\n",
        "    responses.append((paragraph, response))\n",
        "\n",
        "  progressBar(5, 7)\n",
        "  return responses"
      ],
      "metadata": {
        "id": "Yi2XXS4gvIZZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing most relevant sentence\n",
        "def choose_most_relevant_sentence(embeddings, responses, query):\n",
        "  query_embedding = embeddings.encode([query], convert_to_tensor=True)\n",
        "  best_response = None\n",
        "  best_similarity = -1.0\n",
        "  answers = []\n",
        "\n",
        "  for paragraph, response in responses:\n",
        "    # Finding cosine similarity between query embedding and response\n",
        "    similarity = util.pytorch_cos_sim(query_embedding, response).item()\n",
        "\n",
        "    if similarity >= 0.8:\n",
        "      # count += 1\n",
        "      answers.append(paragraph)\n",
        "\n",
        "  answer = \"\\n\".join(answers)\n",
        "\n",
        "  progressBar(6, 7)\n",
        "  return answer"
      ],
      "metadata": {
        "id": "PCdDQy9ywBQz"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying the LLM"
      ],
      "metadata": {
        "id": "nOhbZqLMwSsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query():\n",
        "    query = input(\"Enter your question\\n\")\n",
        "    progressBar(1, 7)\n",
        "    return query"
      ],
      "metadata": {
        "id": "G1qj_Kxswti4"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_the_llm(answer, llm_model, query):\n",
        "    prompt_message = answer + \"\\n\" + query\n",
        "\n",
        "    final_response = llm_model.generate(prompts=[prompt_message])\n",
        "\n",
        "    return final_response"
      ],
      "metadata": {
        "id": "z3qbfLq3wQhc"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the LLM Model\n",
        "def main():\n",
        "  start_time = time.time()\n",
        "  pdf_path = \"./HandbookOfTechnicalAnalysis.pdf\"\n",
        "  pdf_text = load_split_pdf(pdf_path)\n",
        "  paragraphs = split_text_using_RCTS(pdf_text)\n",
        "  embeddings = Initialize_sentence_transformer()\n",
        "  responses = encode_each_paragraph(paragraphs=paragraphs, embeddings=embeddings)\n",
        "  # print(responses)\n",
        "  query = get_query()\n",
        "  answer = choose_most_relevant_sentence(embeddings=embeddings, responses=responses, query=query)\n",
        "\n",
        "  llm = LlamaCpp(\n",
        "      streaming = True,\n",
        "      model_path = \"/content/drive/MyDrive/LLM_Model/mistral-7b-instruct-v0.1.Q3_K_S.gguf\",\n",
        "      temperature = 0.75, # degree of randomness\n",
        "      top_p = 1,\n",
        "      verbose = True,\n",
        "      n_ctx = 4096 # max no. of tokens to generate\n",
        "  )\n",
        "\n",
        "  final_response = query_the_llm(answer=answer, llm_model=llm, query=query)\n",
        "\n",
        "  print (\"The answer from model is\\n\", final_response)\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  print(f\"Execution time: {elapsed_time/60} minutes \\n\")\n",
        "\n",
        "  progressBar(7, 7)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "4IGKtgMaujNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa9755b-14ae-448f-cca7-fd07aa9f2814"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question\n",
            "How many users does Elearnmarkets company have?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /content/drive/MyDrive/LLM_Model/mistral-7b-instruct-v0.1.Q3_K_S.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 11\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q3_K:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q3_K - Small\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 2.95 GiB (3.50 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  3017.27 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 8\n",
            "llama_new_context_with_model: n_ubatch   = 8\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     4.63 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '11'}\n",
            "Using fallback chat format: None\n",
            "\n",
            "llama_print_timings:        load time =    3968.69 ms\n",
            "llama_print_timings:      sample time =      63.03 ms /   101 runs   (    0.62 ms per token,  1602.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7807.86 ms /    14 tokens (  557.70 ms per token,     1.79 tokens per second)\n",
            "llama_print_timings:        eval time =   66622.04 ms /   100 runs   (  666.22 ms per token,     1.50 tokens per second)\n",
            "llama_print_timings:       total time =   74902.01 ms /   114 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer from model is\n",
            " generations=[[Generation(text='\\n\\n## Answer (6)\\n\\nAs of my knowledge up to 2021, elearnmarkets has 3,589 registered users as of July 2018.\\n\\nThis information can be found on the following webpage: https://elearnmarkets.com/about-us/\\n\\nComment: This answer is correct for July 2018 but is incomplete. Please update it to reflect the current state of affairs.')]] llm_output=None run=[RunInfo(run_id=UUID('c05a3cee-268e-4c03-9629-e5dedad13ab8'))]\n",
            "Execution time: 1.8362728794415792 minutes \n",
            "\n",
            "[====================================================================================================] 100.0% ...\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Referenced From: <br>\n",
        "[**Querying a PDF file using LLM models and Sentence transformer**](https://medium.com/@yashashm77/querying-a-pdf-file-using-llm-models-and-sentence-transformer-b3d4d0b40f7d)<br>"
      ],
      "metadata": {
        "id": "5yIn5gOsxbQO"
      }
    }
  ]
}