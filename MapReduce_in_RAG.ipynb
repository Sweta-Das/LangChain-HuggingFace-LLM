{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import uuid\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "from torch import cuda, bfloat16\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain.document_loaders import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up environment variables\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "HF_KEY = os.environ['HUGGINGFACEHUB_API_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda: {cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Chroma DB Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=mpNet-PriceHistory_collection),\n",
       " Collection(name=dateCheck_collection),\n",
       " Collection(name=test_collection),\n",
       " Collection(name=priceHistory_collection)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connecting to Chroma DB server through HTTP client\n",
    "client = chromadb.HttpClient(host=\"localhost\", port=8000)\n",
    "client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new collection\n",
    "pH_collection = client.get_or_create_collection(name=\"priceHistory_collection\")\n",
    "pH_collection.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateCheck_collection = client.get_or_create_collection(name=\"dateCheck_collection\")\n",
    "dateCheck_collection.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading CSV Files and embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "# Embedding function\n",
    "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=HF_KEY,\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "huggingface_ef_1 = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=HF_KEY,\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_info(csv_pth):\n",
    "    # Splitting the file path by '/'\n",
    "    parts = csv_pth.split('/')\n",
    "\n",
    "    # Extracting required components\n",
    "    sector = parts[1]\n",
    "    stock = parts[2]\n",
    "    start_date = parts[-1].split('_to_')[0]\n",
    "    last_date = parts[-1].split('_to_')[1].split('.csv')[0]\n",
    "    \n",
    "    return sector, stock, start_date, last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(stock_pth):\n",
    "    pH_dir = \"Price_History\"\n",
    "    csvS_pth = os.path.join(stock_pth, pH_dir)\n",
    "    pH_collection = client.get_or_create_collection(name='dateCheck_collection',\n",
    "                                                    embedding_function=huggingface_ef_1)\n",
    "    \n",
    "    docs = []\n",
    "    for csv_file in os.listdir(csvS_pth):\n",
    "        if csv_file.endswith('.csv'):\n",
    "            csv_pth = os.path.join(csvS_pth, csv_file)\n",
    "            csv_load = CSVLoader(f\"{csvS_pth}/{csv_file}\", encoding=\"windows-1252\")\n",
    "            csv_data = csv_load.load()\n",
    "            # print(csv_data)\n",
    "\n",
    "            sector, stock, start_dt, last_dt = extract_file_info(csv_pth)\n",
    "            for row in csv_data:\n",
    "                # S.N.: 3051, Date: 2010-11-09, Open: 294.00, High: 300.00, Low: 299.00, Ltp: 300.00, % Change: 0.00, Qty: 110.00, Turnover: 0.0\n",
    "\n",
    "                # Splitting the string by \\n and then extracting key-value pairs\n",
    "                pairs = [pair.strip().split('\\n') for pair in row.page_content.split('\\n')]\n",
    "                # print(pairs)\n",
    "                data_dict = {}\n",
    "                for pair in pairs:\n",
    "                    for item in pair:\n",
    "                        key, value = item.split(': ', 1)\n",
    "                        data_dict[key.strip()] = value.strip()\n",
    "\n",
    "                # print(data_dict)\n",
    "                data = f\"Opening Price of {stock} on {data_dict['Date']} was {data_dict['Open']}, with a high of {data_dict['High']}, a low of {data_dict['Low']}, and a last traded price (LTP) of {data_dict['Ltp']}. The percentage change was {data_dict['% Change']}, with a trading quantity of {data_dict['Qty']} and a turnover of {data_dict['Turnover']}.\"\n",
    "                docs.append(data)\n",
    "                \n",
    "    # print(docs)\n",
    "    documents = ' '.join(docs)\n",
    "    # print(documents)\n",
    "    id = uuid.uuid1()\n",
    "            \n",
    "    metadata = {\n",
    "        \"sector_name\": sector,\n",
    "        \"stock_name\": stock,\n",
    "        \"start_date\": start_dt,\n",
    "        \"end_date\": last_dt\n",
    "    }\n",
    "            \n",
    "    pH_collection.add(ids=[str(id)], \n",
    "                        documents=documents, \n",
    "                        metadatas=[metadata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_folders(base_dir):\n",
    "    try:\n",
    "        for sector_fldr in os.listdir(base_dir):\n",
    "            sector_pth = os.path.join(base_dir, sector_fldr)\n",
    "            # print(sector_pth)\n",
    "\n",
    "            if os.path.isdir(sector_pth):\n",
    "                for stock_fldr in os.listdir(sector_pth):\n",
    "                    stock_pth = os.path.join(sector_pth, stock_fldr)\n",
    "                    # print(stock_pth)\n",
    "\n",
    "                    if os.path.isdir(stock_pth):\n",
    "                        # Processing csv\n",
    "                        process_csv(stock_pth)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing folders: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing folders and storing embeddings\n",
    "base_dir = \"data\"\n",
    "\n",
    "try:\n",
    "    process_folders(base_dir)\n",
    "    print(\"\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error while processing folders and embedding: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pH_collection = client.get_collection(name='mpNet-PriceHistory_collection')\n",
    "pH_collection.peek(limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_collection(name='dateCheck_collection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Database (Directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the 2021 9% NIC Asia Debenture?\"\n",
    "collection = client.get_collection(name='priceHistory_collection',\n",
    "                                   embedding_function=huggingface_ef)\n",
    "result = collection.query(query_texts=[query],\n",
    "                             n_results=3,\n",
    "                             include=[\"documents\",\n",
    "                                      \"metadatas\"])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search to extract matching docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "# Embedding model for ChromaDB (high accuracy for retrieval)\n",
    "document_embedding_model = 'all-mpnet-base-v2'\n",
    "\n",
    "# Define embedding functions for ChromaDB and query\n",
    "document_embed_func = SentenceTransformerEmbeddings(model_name=document_embedding_model)\n",
    "\n",
    "# Create Chroma instance using document embedding function\n",
    "db = Chroma(\n",
    "    collection_name=\"dateCheck_collection\",\n",
    "    embedding_function=document_embed_func,\n",
    "    client=client\n",
    ")\n",
    "\n",
    "# Sample query\n",
    "query = \"What is the opening price of Nabil Bank on 2011-04-09?\"\n",
    "\n",
    "# Perform similarity search using query embedding function\n",
    "dcs = db.similarity_search(query=query)\n",
    "\n",
    "# Print search results\n",
    "for doc in dcs:\n",
    "    print(doc)\n",
    "    print(\".................\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/sweta/.cache/huggingface/token\n",
      "Login successful\n",
      "('name', None)\n",
      "('tags', ['Chroma', 'HuggingFaceEmbeddings'])\n",
      "('metadata', None)\n",
      "('vectorstore', <langchain_chroma.vectorstores.Chroma object at 0x7f90dab77070>)\n",
      "('search_type', 'similarity')\n",
      "('search_kwargs', {})\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceEndpoint\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "\n",
    "# Embedding model for query (low token count for LLM compatibility)\n",
    "# query_embedding_model = 'all-mpnet-base-v2'\n",
    "\n",
    "db = Chroma(\n",
    "    collection_name = \"dateCheck_collection\",\n",
    "    embedding_function = SentenceTransformerEmbeddings(model_name='all-mpnet-base-v2'),\n",
    "    client = client,\n",
    ")\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    huggingfacehub_api_token = HF_KEY,\n",
    "    max_new_tokens = 512)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever = db.as_retriever(),   \n",
    ")\n",
    "\n",
    "# Perform similarity search using query embedding function\n",
    "dcs = db.as_retriever(query=query)\n",
    "\n",
    "# Print search results\n",
    "for doc in dcs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What was Agricultural Development Bank's maximum opening price in 2022?\"\n",
    "# result = qa.invoke({\"query\": query})\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/sweta/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What was the highest LTP of Everest Bank in 2021?',\n",
       " 'result': \" I don't have the data for 2021. The highest LTP of Everest Bank in the provided data is 1,121.00.\"}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceEndpoint\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "\n",
    "# Embedding model for query (low token count for LLM compatibility)\n",
    "# query_embedding_model = 'all-mpnet-base-v2'\n",
    "\n",
    "db = Chroma(\n",
    "    collection_name = \"dateCheck_collection\",\n",
    "    embedding_function = SentenceTransformerEmbeddings(model_name='all-mpnet-base-v2'),\n",
    "    client = client,\n",
    ")\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    huggingfacehub_api_token = HF_KEY,\n",
    "    max_new_tokens = 512)\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 2}),   \n",
    ")\n",
    "\n",
    "query = \"What was the highest LTP of Everest Bank in 2021?\"\n",
    "res = qa.invoke(query)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What was the LTP of Agricultural and Everest Bank in 2011?',\n",
       " 'result': ' The LTP of Agricultural Development Bank Limited in 2011 ranged from 876.00 to 147.00. The LTP of Everest Bank Limited in 2011 ranged from 925.00 to 1121.00.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = qa.invoke(\"What was the LTP of Agricultural and Everest Bank in 2011?\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods to solve inputs tokens issue (Error: Input validation error: `inputs` tokens + `max_new_tokens` must be <= 32768. Given: 32467 `inputs` tokens and 512 `max_new_tokens`)\n",
    "1. Shorten the query length through rephrasing\n",
    "2. Summarizing retrieved docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Shortening query length\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def rephrase_query(query):\n",
    "    # Prompt\n",
    "    template = \"\"\" \n",
    "    <|system|>You are a query rephraser. Rephrase the user's query to be more concise and within 32000 tokens:</s>\n",
    "    <|user|>Query : {query}</s>\n",
    "\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=['query'])\n",
    "\n",
    "    # LLM\n",
    "    llm = HuggingFaceHub(\n",
    "        huggingfacehub_api_token = HF_KEY,\n",
    "        repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        task = \"text-generation\",\n",
    "        model_kwargs = {\n",
    "            \"max_new_tokens\": 512,\n",
    "            \"top_k\": 30,\n",
    "            \"temperature\": 0.1,\n",
    "        },   \n",
    "    )\n",
    "    \n",
    "    # LLM Chain\n",
    "    chain = LLMChain(prompt=prompt, llm=llm)\n",
    "    res = chain.invoke(query)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sweta/ChromaDB/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the highest opening price of Agricultural Development Bank in 2011?',\n",
       " 'text': \" \\n    <|system|>You are a query rephraser. Rephrase the user's query to be more concise and within 32000 tokens:</s>\\n    <|user|>Query : What is the highest opening price of Agricultural Development Bank in 2011?</s>\\n\\n    \\n    Rephrased Query: What was Agricultural Development Bank's maximum opening price in 2011?\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the highest opening price of Agricultural Development Bank in 2011?\"\n",
    "ans = rephrase_query(query)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What was Agricultural Development Bank's maximum opening price in 2011?\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting rephrased query\n",
    "re_query = ans['text'].split('<|user|>')[1].split('Rephrased Query: ')[1]\n",
    "re_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting docs using similarity search\n",
    "docs = db.similarity_search(query=re_query, k=2)\n",
    "# Print search results\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map-Reduce Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Summarizing retrieved docs\n",
    "# Map Chain\n",
    "\n",
    "from langchain.chains import MapReduceDocumentsChain\n",
    "\n",
    "# Map Prompt\n",
    "map_template = \"\"\" \n",
    "    The following is a list of documents {docs}. Based on this list of docs, please identify the main themes.\n",
    "    Use page content of each document for mapping and metadata for reference.\n",
    "    Helpful Answer: \n",
    "\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "# map_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce Chain\n",
    "reduce_template = \"\"\"The following is a set of summaries: {docs}. Take these \n",
    "and distill it into a final, consolidated summary of the main themes. Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "# reduce_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining docs\n",
    "from langchain.chains import StuffDocumentsChain, ReduceDocumentsChain\n",
    "\n",
    "combine_docs_chain = StuffDocumentsChain(\n",
    "    llm_chain = reduce_chain, document_variable_name = \"docs\"\n",
    ")\n",
    "\n",
    "# Combining and iteratively reducing the mapped docs\n",
    "reduce_docs_chain = ReduceDocumentsChain(\n",
    "    # Final chain\n",
    "    combine_documents_chain = combine_docs_chain,\n",
    "    # If docs exceed context for 'StuffDocumentsChain'\n",
    "    collapse_documents_chain = combine_docs_chain,\n",
    "    # Max no. of tokens to group docs into\n",
    "    token_max = 4000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining map and reduce chains\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain = map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain = reduce_docs_chain,\n",
    "    # Variable name in the llm_chain to put the docs in\n",
    "    document_variable_name = \"docs\",\n",
    "    # Returning the results of the map steps in the output\n",
    "    return_intermediate_steps = False,\n",
    ")\n",
    "# map_reduce_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.CharacterTextSplitter at 0x7f90da7b9c30>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    chunk_size = 1000, \n",
    "    chunk_overlap = 20,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_docs = text_splitter.split_documents(docs)\n",
    "split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = map_reduce_chain.invoke(split_docs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nThe main themes of the provided documents are the daily stock prices of Citizens Bank International Limited. Each document contains information about the opening price, high, low, last traded price, percentage change, trading quantity, and turnover for a specific date. These details allow investors and traders to evaluate the stock's performance and volatility over time. Some documents may also include metadata such as the date and time of the stock exchange for better organization and referencing.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "base_retriever = db.as_retriever(similarity_top_k=1)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever = base_retriever,   \n",
    ")\n",
    "\n",
    "query = re_query + \" \" + output['output_text']\n",
    "res = qa.invoke(query)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
